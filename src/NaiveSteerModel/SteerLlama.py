import logging

import torch.nn as nn
import torch
from transformers import LlamaForCausalLM, LlamaModel, LlamaConfig
from transformers.models.llama.modeling_llama import LlamaDecoderLayer


# Add these imports
from typing import Optional, Tuple, Union, List, Dict#, Unpack
from transformers.cache_utils import Cache
from utils.mask_utils import get_last_valid_token_index


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)



# Only CausalLM can be called from outside
__all__ = [
    'SteerLlamaForCausalLM',
    ]

class SteerLlamaDecoderLayer(LlamaDecoderLayer):
    def __init__(self, config: LlamaConfig, 
                 layer_idx: int, 
                 steering_vector: Optional[torch.Tensor] = None, 
                 strength: float = 1.0
                 ):
        super().__init__(config, layer_idx)
        self.layer_idx = layer_idx
        
        device = next(self.parameters()).device
        dtype = self.input_layernorm.weight.dtype
        hidden_dim = config.hidden_size
        if steering_vector is not None:
            self.steering_vector = steering_vector.to(device=device, dtype=dtype)
        else:
            self.steering_vector = torch.empty(hidden_dim, device=device, dtype=dtype)
        strength = 0.0 if strength is None else strength
        self.strength = torch.tensor(strength, device=device, dtype=dtype)

    def set_steering_parameters(
        self, 
        steering_vector: Optional[torch.Tensor]=None, 
        strength: float = 0.0,
        device: Optional[torch.device] = None):
        
        device = next(self.parameters()).device if device is None else device
        dtype = self.input_layernorm.weight.dtype
        
        if steering_vector is not None:
            self.steering_vector = steering_vector.to(device=device, dtype=dtype)
        else:
            self.steering_vector = None

        strength = 0.0 if strength is None else strength
        self.strength = torch.tensor(strength, device=device, dtype=dtype)
        

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs#: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        
        ############################################################
        if (
            hidden_states.shape[1] > 1
            and self.steering_vector is not None
            and torch.any(self.steering_vector)
            and self.strength != 0.0
        ):
            if self.steering_vector.device != hidden_states.device:
                self.steering_vector = self.steering_vector.to(hidden_states.device)
            if self.strength.device != hidden_states.device:
                self.strength = self.strength.to(hidden_states.device)

            # B, T, _ = hidden_states.shape
            # device = hidden_states.device
            # last_idx = get_last_valid_token_index(
            #     attention_mask=attention_mask,
            #     seq_len=T,
            #     batch_size=B,
            #     device=device,
            # )
            # batch_idx = torch.arange(B, device=device)
            # hidden_states[batch_idx, last_idx, :] += self.steering_vector * self.strength
            hidden_states = hidden_states + self.steering_vector * self.strength
        ############################################################
        
        residual = hidden_states # resid_pre
        hidden_states = self.input_layernorm(hidden_states)

        hidden_states, self_attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        
        hidden_states = residual + hidden_states
        residual = hidden_states # resid_mid
        
        # Normalize the hidden states after attention, then pass through MLP
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        
        # resid_post
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)
        return outputs


class SteerLlamaModel(LlamaModel):
    def __init__(self, config: LlamaConfig, 
                 steering_vector: Optional[torch.Tensor] = None,
                 strength: Optional[list[float]] = None):
        super().__init__(config)
        # Replace this, keep the rest unchanged
        self.steering_vector = steering_vector
        self.strength = strength if strength is not None else [0.0] * config.num_hidden_layers
        self.layers = nn.ModuleList(
            [SteerLlamaDecoderLayer(
                config=config, 
                layer_idx=layer_idx, 
                steering_vector=steering_vector[layer_idx] if steering_vector is not None else None, 
                strength=self.strength[layer_idx] if self.strength is not None else 0.0,
            )
             for layer_idx in range(config.num_hidden_layers)]
        )

    def set_steering_parameters(
        self, 
        steering_vector: Optional[torch.Tensor] = None, 
        strength: Optional[list[float]] = None,
        device: Optional[torch.device] = None):
        
        device = next(self.parameters()).device if device is None else device
        
        if steering_vector is not None:
            steering_vector = steering_vector.to(device)
            
        for layer_idx, layer in enumerate(self.layers):
            layer_steering_vector = None
            if steering_vector is not None:
                layer_steering_vector = steering_vector[layer_idx]

            layer.set_steering_parameters(
                steering_vector=layer_steering_vector, 
                strength=strength[layer_idx] if strength is not None else 0.0
            )
            torch.cuda.empty_cache()
        self.print_steering_parameters()

    def print_steering_parameters(self):
        logger.info(f"{'Layer':<8}{'Strength':<12}{'Steering Vector'}")
        logger.info("-" * 32)
        for layer_idx, layer in enumerate(self.layers):
            strength = f"{layer.strength:.4f}" if isinstance(layer.strength, float) else f"{layer.strength.item():.4f}"
            vector = "None" if layer.steering_vector is None or layer.steering_vector.nelement() == 0 else f"{layer.steering_vector[0].item():.4f}"
            logger.info(f"{layer_idx:<8}{strength:<12}{vector}")


class SteerLlamaForCausalLM(LlamaForCausalLM):
    def __init__(self, config: LlamaConfig, 
                 steering_vector: Optional[torch.Tensor] = None, # This is 2D [layer_idx, d_model], initialized as 0
                 strength: Optional[list[float]] = None):
        super().__init__(config)
        self.model = SteerLlamaModel(
            config=config, 
            steering_vector=steering_vector, 
            strength=strength
        )

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, 
                        steering_vector: Optional[torch.Tensor] = None,
                        strength: Optional[list[float]] = None,
                        **kwargs):
        # Call the parent class's from_pretrained method to load the model
        model = super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
        model.set_steering_parameters(steering_vector=steering_vector, strength=strength)
        return model

    def set_steering_parameters(
            self, 
            steering_vector: Optional[torch.Tensor] = None, 
            strength: Optional[list[float]] = None):
        
        # Ensure steering_matrix is on the device where the model is
        device = next(self.parameters()).device
        if steering_vector is not None:
            steering_vector = steering_vector.to(device)

        self.model.set_steering_parameters(
            steering_vector=steering_vector, 
            strength=strength,
            device=device
        )
